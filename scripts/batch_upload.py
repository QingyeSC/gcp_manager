#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡ä¸Šä¼ è„šæœ¬ - åŸºäºç”¨æˆ·åŸå§‹ä»£ç æ”¹è¿›
åŠŸèƒ½ï¼šä»è´¦å·æ± ä¸­é€‰æ‹©å¹¶ä¸Šä¼ JSONæ–‡ä»¶åˆ°New API
"""

import json
import requests
import os
import time
import sys
import argparse
import random
import re
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime

# é…ç½®æ—¥å¿—
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/batch_upload.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BatchUploader:
    """æ‰¹é‡ä¸Šä¼ ç®¡ç†ç±»"""
    
    def __init__(self, config_path="config/settings.json"):
        self.load_config(config_path)
        self.base_dir = Path("accounts")
        
        # æ²¿ç”¨ç”¨æˆ·çš„å›ºå®špayloadé…ç½®
        self.fixed_payload = {
            "type": 41,
            "openai_organization": "",
            "max_input_tokens": 0,
            "base_url": "",
            "other": "{\n  \"default\": \"us-central1\",\n  \"gemini-2.5-flash-lite-preview-06-17\": \"global\",\n  \"gemini-2.5-pro-exp-03-25\": \"global\",\n  \"gemini-2.5-pro-preview-06-05\": \"global\"\n}",
            "model_mapping": "",
            "status_code_mapping": "",
            "models": "gemini-2.5-pro-exp-03-25,gemini-2.0-flash-001,gemini-2.5-flash-preview-04-17,gemini-2.5-flash-preview-05-20,gemini-2.5-pro-preview-03-25,gemini-2.5-pro-preview-05-06,gemini-2.5-pro-preview-06-05,gemini-2.5-flash,gemini-2.5-flash-lite-preview-06-17,gemini-2.5-pro",
            "auto_ban": 1,
            "test_model": "gemini-2.5-pro",
            "groups": ["default", "vip", "svip"],
            "priority": 1,
            "weight": 1,
            "tag": "Vertex-vip",
            "group": "default,vip,svip"
        }
        
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    
    def load_config(self, config_path):
        """åŠ è½½é…ç½®"""
        if Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            self.api_url = config['new_api']['base_url'] + config['new_api']['upload_endpoint']
            self.api_token = config['new_api']['api_key']
        else:
            # ä»ç¯å¢ƒå˜é‡è·å–
            base_url = os.getenv('NEW_API_BASE_URL', 'http://152.53.166.175:3058')
            self.api_url = base_url + '/api/channel/'
            self.api_token = os.getenv('NEW_API_TOKEN', '')
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            "Authorization": f"Bearer {self.api_token}",
            "Content-Type": "application/json",
            "New-API-User": "1"
        }
        
        if not self.api_token:
            logger.error("è¯·è®¾ç½® NEW_API_TOKEN")
            sys.exit(1)
    
    def parse_arguments(self):
        """è§£æå‘½ä»¤è¡Œå‚æ•°"""
        parser = argparse.ArgumentParser(description='æ‰¹é‡ä¸Šä¼ JSONæ–‡ä»¶åˆ°é¢‘é“')
        parser.add_argument('upload_count', type=int, nargs='?', default=3,
                            help='è¦ä¸Šä¼ çš„æ–‡ä»¶æ•°é‡ï¼ˆå¿…é¡»æ˜¯3çš„å€æ•°ï¼Œé»˜è®¤ä¸º3ï¼‰')
        parser.add_argument('--source', default='fresh',
                            help='æºç›®å½•åç§°ï¼Œé»˜è®¤ä¸ºfresh')
        args = parser.parse_args()
        
        # æ£€æŸ¥æ•°é‡æ˜¯å¦ä¸º3çš„å€æ•°
        if args.upload_count % 3 != 0:
            logger.error(f"ä¸Šä¼ æ•°é‡å¿…é¡»æ˜¯3çš„å€æ•°ï¼Œä½ è¾“å…¥çš„æ˜¯ {args.upload_count}")
            sys.exit(1)
        
        if args.upload_count <= 0:
            logger.error(f"ä¸Šä¼ æ•°é‡å¿…é¡»å¤§äº0ï¼Œä½ è¾“å…¥çš„æ˜¯ {args.upload_count}")
            sys.exit(1)
        
        return args.upload_count, args.source
    
    def get_project_groups(self, source_dir):
        """è·å–æŒ‡å®šç›®å½•å†…æ‰€æœ‰JSONæ–‡ä»¶å¹¶æŒ‰é¡¹ç›®åˆ†ç»„"""
        json_folder = self.base_dir / source_dir
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not json_folder.exists():
            logger.error(f"æ‰¾ä¸åˆ° {json_folder} ç›®å½•ï¼")
            return {}
        
        if not json_folder.is_dir():
            logger.error(f"{json_folder} ä¸æ˜¯ä¸€ä¸ªç›®å½•ï¼")
            return {}
        
        # è·å–ç›®å½•å†…çš„JSONæ–‡ä»¶
        try:
            json_files = [f for f in json_folder.iterdir() if f.suffix == '.json']
        except Exception as e:
            logger.error(f"æ— æ³•è¯»å– {json_folder} ç›®å½•ï¼š{e}")
            return {}
        
        if not json_files:
            logger.error(f"åœ¨ {json_folder} ç›®å½•å†…æ²¡æœ‰æ‰¾åˆ°ä»»ä½•JSONæ–‡ä»¶ï¼")
            return {}
        
        logger.info(f"åœ¨ {json_folder} ç›®å½•å†…æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        
        # æŒ‰é¡¹ç›®åˆ†ç»„
        groups = {}
        
        for file in json_files:
            # åŒ¹é…æ–‡ä»¶åæ¨¡å¼ï¼Œä¾‹å¦‚ï¼šproj-roble-vip-01.json
            # æå–é¡¹ç›®å‰ç¼€ï¼ˆå»æ‰æœ€åçš„æ•°å­—éƒ¨åˆ†ï¼‰
            match = re.match(r'(.+)-(\d+)\.json', file.name)
            if match:
                project_prefix = match.group(1)  # ä¾‹å¦‚ï¼šproj-roble-vip
                number = match.group(2)          # ä¾‹å¦‚ï¼š01
                
                if project_prefix not in groups:
                    groups[project_prefix] = []
                groups[project_prefix].append(file)
            else:
                logger.warning(f"æ–‡ä»¶ {file.name} ä¸ç¬¦åˆå‘½åè§„èŒƒï¼Œè·³è¿‡")
        
        # åªä¿ç•™å®Œæ•´çš„ç»„ï¼ˆåŒ…å«3ä¸ªæ–‡ä»¶çš„ç»„ï¼‰
        complete_groups = {}
        for project, files in groups.items():
            if len(files) == 3:
                complete_groups[project] = sorted(files)  # æ’åºç¡®ä¿é¡ºåºä¸€è‡´
            else:
                logger.warning(f"é¡¹ç›® {project} åªæœ‰ {len(files)} ä¸ªæ–‡ä»¶ï¼Œéœ€è¦3ä¸ªæ–‡ä»¶æ‰èƒ½æ„æˆå®Œæ•´ç»„ï¼Œè·³è¿‡")
        
        return complete_groups
    
    def select_upload_groups(self, groups, upload_count, prefer_activated=True):
        """é€‰æ‹©è¦ä¸Šä¼ çš„é¡¹ç›®ç»„"""
        if not groups:
            return []
        
        groups_needed = upload_count // 3
        available_groups = list(groups.keys())
        
        if len(available_groups) < groups_needed:
            logger.warning(f"åªæœ‰ {len(available_groups)} ä¸ªå®Œæ•´é¡¹ç›®ç»„ï¼Œä½†éœ€è¦ {groups_needed} ä¸ªç»„")
            groups_needed = len(available_groups)
        
        # å¦‚æœåå¥½æ¿€æ´»è´¦å·ï¼Œä¼˜å…ˆé€‰æ‹©å¸¦-activedçš„
        if prefer_activated:
            activated_groups = [g for g in available_groups if any('-actived' in f.name for f in groups[g])]
            fresh_groups = [g for g in available_groups if g not in activated_groups]
            
            selected_projects = []
            
            # å…ˆé€‰æ‹©æ¿€æ´»è´¦å·
            if activated_groups and groups_needed > 0:
                take_activated = min(len(activated_groups), groups_needed)
                selected_projects.extend(random.sample(activated_groups, take_activated))
                groups_needed -= take_activated
            
            # å†é€‰æ‹©æ–°è´¦å·
            if fresh_groups and groups_needed > 0:
                take_fresh = min(len(fresh_groups), groups_needed)
                selected_projects.extend(random.sample(fresh_groups, take_fresh))
        else:
            # éšæœºé€‰æ‹©é¡¹ç›®ç»„
            selected_projects = random.sample(available_groups, groups_needed)
        
        # è·å–é€‰ä¸­é¡¹ç›®ç»„çš„æ‰€æœ‰æ–‡ä»¶
        selected_files = []
        for project in selected_projects:
            selected_files.extend(groups[project])
            # æ˜¾ç¤ºç›¸å¯¹è·¯å¾„ï¼Œæ›´ç®€æ´
            file_names = [f.name for f in groups[project]]
            logger.info(f"é€‰ä¸­é¡¹ç›®ç»„ï¼š{project} -> {file_names}")
        
        return selected_files
    
    def escape_json_content(self, file_path):
        """è½¬ä¹‰JSONæ–‡ä»¶å†…å®¹"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return json.dumps(json.loads(content))  # æ ‡å‡†JSONè½¬ä¹‰
    
    def upload_channel(self, file_path):
        """ä¸Šä¼ å•ä¸ªé¢‘é“æ–‡ä»¶"""
        name = file_path.stem
        try:
            key_content = self.escape_json_content(file_path)
            payload = self.fixed_payload.copy()
            payload["name"] = name
            payload["key"] = key_content

            for attempt in range(1, self.max_retries + 1):
                try:
                    response = requests.post(self.api_url, headers=self.headers, json=payload, timeout=15)
                    if response.status_code == 200:
                        return (name, file_path, True, "")
                    else:
                        if attempt < self.max_retries:
                            logger.warning(f"[é‡è¯•] {name} ç¬¬{attempt}æ¬¡å¤±è´¥ï¼ŒçŠ¶æ€ç {response.status_code}ï¼Œå‡†å¤‡é‡è¯•...")
                            time.sleep(1)
                        else:
                            return (name, file_path, False, f"çŠ¶æ€ç  {response.status_code}ï¼Œè¿”å›: {response.text}")
                except Exception as e:
                    if attempt < self.max_retries:
                        logger.warning(f"[é‡è¯•] {name} ç¬¬{attempt}æ¬¡å¼‚å¸¸ {e}ï¼Œå‡†å¤‡é‡è¯•...")
                        time.sleep(1)
                    else:
                        return (name, file_path, False, f"å¼‚å¸¸: {str(e)}")
        except Exception as e:
            return (name, file_path, False, f"è§£ææˆ–å‡†å¤‡ä¸Šä¼ å¼‚å¸¸: {str(e)}")
    
    def move_uploaded_files(self, success_files, target_dir="uploaded"):
        """ç§»åŠ¨ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•"""
        target_path = self.base_dir / target_dir
        target_path.mkdir(parents=True, exist_ok=True)
        
        moved_files = []
        failed_move_files = []
        
        for file_path in success_files:
            try:
                destination = target_path / file_path.name
                shutil.move(str(file_path), str(destination))
                moved_files.append(destination)
                logger.info(f"[ç§»åŠ¨æˆåŠŸ] {file_path.name} -> {target_dir}/")
            except Exception as e:
                failed_move_files.append((file_path, str(e)))
                logger.error(f"[ç§»åŠ¨å¤±è´¥] {file_path.name}ï¼ŒåŸå› ï¼š{e}")
        
        return moved_files, failed_move_files
    
    def run_upload(self, upload_count, source_dir):
        """æ‰§è¡Œä¸Šä¼ ä»»åŠ¡"""
        logger.info("=" * 50)
        logger.info("æ‰¹é‡ä¸Šä¼ ä»»åŠ¡å¼€å§‹")
        logger.info(f"æœ¬æ¬¡å°†ä¸Šä¼  {upload_count} ä¸ªæ–‡ä»¶ï¼ˆ{upload_count//3} ä¸ªé¡¹ç›®ç»„ï¼‰")
        
        # è·å–é¡¹ç›®åˆ†ç»„
        logger.info(f"æ­£åœ¨æ‰«æ {source_dir} ç›®å½•å†…çš„JSONæ–‡ä»¶...")
        groups = self.get_project_groups(source_dir)
        
        if not groups:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å®Œæ•´çš„é¡¹ç›®ç»„ï¼")
            return False
        
        logger.info(f"æ‰¾åˆ° {len(groups)} ä¸ªå®Œæ•´é¡¹ç›®ç»„ï¼š")
        for project, files in groups.items():
            file_names = [f.name for f in files]
            logger.info(f"  - {project}: {file_names}")
        
        # é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶
        logger.info(f"æ­£åœ¨é€‰æ‹© {upload_count//3} ä¸ªé¡¹ç›®ç»„...")
        prefer_activated = (source_dir == "activated")
        selected_files = self.select_upload_groups(groups, upload_count, prefer_activated)
        
        if not selected_files:
            logger.error("æ²¡æœ‰å¯ä¸Šä¼ çš„æ–‡ä»¶ï¼")
            return False
        
        logger.info(f"å³å°†ä¸Šä¼ ä»¥ä¸‹ {len(selected_files)} ä¸ªæ–‡ä»¶ï¼š")
        for file in selected_files:
            logger.info(f"  - {file.name}")
        
        # å¼€å§‹ä¸Šä¼ 
        logger.info("å¼€å§‹ä¸Šä¼ ...")
        success_list = []
        fail_list = []
        success_files = []  # ç”¨äºè®°å½•æˆåŠŸä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„

        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(self.upload_channel, file) for file in selected_files]
            for future in as_completed(futures):
                name, file_path, success, message = future.result()
                if success:
                    success_list.append(name)
                    success_files.append(file_path)
                    logger.info(f"[æˆåŠŸ] {name}")
                else:
                    fail_list.append((name, message))
                    logger.error(f"[å¤±è´¥] {name}ï¼ŒåŸå› ï¼š{message}")

        # ç§»åŠ¨ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶
        if success_files:
            logger.info(f"å¼€å§‹ç§»åŠ¨ä¸Šä¼ æˆåŠŸçš„ {len(success_files)} ä¸ªæ–‡ä»¶...")
            moved_files, failed_move_files = self.move_uploaded_files(success_files)
        else:
            moved_files = []
            failed_move_files = []

        # æ‰“å°æ€»ç»“
        logger.info("====== æ€»ç»“ ======")
        logger.info(f"æˆåŠŸä¸Šä¼  {len(success_list)} ä¸ªï¼š{success_list}")
        logger.info(f"å¤±è´¥ä¸Šä¼  {len(fail_list)} ä¸ªï¼š")
        for name, reason in fail_list:
            logger.info(f"  - {name}: {reason}")
        
        if moved_files:
            moved_file_names = [f.name for f in moved_files]
            logger.info(f"æˆåŠŸç§»åŠ¨ {len(moved_files)} ä¸ªæ–‡ä»¶ï¼š{moved_file_names}")
        
        if failed_move_files:
            logger.info(f"ç§»åŠ¨å¤±è´¥ {len(failed_move_files)} ä¸ªæ–‡ä»¶ï¼š")
            for file_path, reason in failed_move_files:
                logger.info(f"  - {file_path.name}: {reason}")

        # ä¿å­˜æ—¥å¿—
        self.save_upload_log(upload_count, selected_files, success_list, fail_list, moved_files, failed_move_files)
        
        return len(success_list) > 0
    
    def save_upload_log(self, upload_count, selected_files, success_list, fail_list, moved_files, failed_move_files):
        """ä¿å­˜ä¸Šä¼ æ—¥å¿—"""
        log_path = Path("logs/upload_detailed.log")
        log_path.parent.mkdir(exist_ok=True)
        
        with open(log_path, "a", encoding="utf-8") as log_file:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            log_file.write(f"\n===== ä¸Šä¼ æ—¥å¿— {timestamp} =====\n")
            log_file.write(f"ä¸Šä¼ ç›®æ ‡æ•°é‡ï¼š{upload_count} ä¸ªæ–‡ä»¶\n")
            selected_file_names = [f.name for f in selected_files]
            log_file.write(f"å®é™…é€‰æ‹©æ–‡ä»¶ï¼š{selected_file_names}\n\n")
            
            log_file.write(f"æˆåŠŸä¸Šä¼  {len(success_list)} ä¸ªï¼š\n")
            for name in success_list:
                log_file.write(f"[æˆåŠŸ] {name}\n")

            log_file.write(f"\nå¤±è´¥ä¸Šä¼  {len(fail_list)} ä¸ªï¼š\n")
            for name, reason in fail_list:
                log_file.write(f"[å¤±è´¥] {name}ï¼ŒåŸå› ï¼š{reason}\n")
            
            if moved_files:
                log_file.write(f"\næˆåŠŸç§»åŠ¨ {len(moved_files)} ä¸ªæ–‡ä»¶ï¼š\n")
                for file_path in moved_files:
                    log_file.write(f"[ç§»åŠ¨æˆåŠŸ] {file_path.name}\n")
            
            if failed_move_files:
                log_file.write(f"\nç§»åŠ¨å¤±è´¥ {len(failed_move_files)} ä¸ªæ–‡ä»¶ï¼š\n")
                for file_path, reason in failed_move_files:
                    log_file.write(f"[ç§»åŠ¨å¤±è´¥] {file_path.name}ï¼ŒåŸå› ï¼š{reason}\n")

def main():
    uploader = BatchUploader()
    
    try:
        upload_count, source_dir = uploader.parse_arguments()
        success = uploader.run_upload(upload_count, source_dir)
        
        if success:
            logger.info("ğŸ‰ ä¸Šä¼ ä»»åŠ¡å®Œæˆ!")
            sys.exit(0)
        else:
            logger.error("âŒ ä¸Šä¼ ä»»åŠ¡å¤±è´¥!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("ä¸Šä¼ ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ä¸Šä¼ ä»»åŠ¡å¼‚å¸¸: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()